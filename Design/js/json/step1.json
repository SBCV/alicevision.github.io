{
  "nameStep": "1 - Structure From Motion",
  "nameSecond" : "1- Structure From Motion",
  "objectives" : "Objectives",
  "objectivesP" : "Extract 2D points of interest from input images, in order to get image keypoints, corresponding to salient 3D points.",
  "introStep": "Feature Extraction de coins, de lignes, de contraste : utiliser dans du multi-view, du coup quand on ajoute une nouvelle photo, essayer de repérer ces mêmes points dans la scène.<br/><br/>Feature Extraction de coins, de lignes, de contrasteBut : utiliser dans du multi-view, du coup quand on ajoute une nouvelle photo.",
  "entry": "Images",
  "exit" : "2D points of interest for each images",
  "generalPrincipeIMG":"img_step1.png",

  "step1Name": "Step 1 : Features Detector",
  "step1Content":"Find points of interest in all the images that are irrespective of rotation, translation, and scale.",
  "step2Name": "Step 2 : Filtering",
  "step2Content":"Selection of the most high-level features while also ensuring repartition in the image.",
  "step3Name": "Step 3 : Descriptor Extractor",
  "step3Content":"Generate a descriptor to stock these data as numerical features.",
  "step4Name": "Optional Step : Masking Optimization",
  "step4Content": "Use a binary image mask to only extract features in a subpart of the image.",
   "algo1H2":"The implemented algorithm",

   "algo1":"SIFT (Scale Invariant Feature Transform)",
   "algo1ATitle":"1- Scale-space Extrema Detection",
   "algo1Ap1":"The initial goal of SIFT is to extract discriminative patches in a first image that can be compared to discriminative patches of a second image irrespective of rotation, translation, and scale. As a relevant detail only exists at a certain scale, the extracted patches are centered at stable points of interest which are well-known to lie at extrema of some specific image energy-based representation ---so-called Laplacian--- in the image scale-space representation. The key idea is that, to some extent, one can use the SIFT invariance to deal with the image transformations occurring when the viewpoints are changing during image acquisition.",
   "algo1Ap2":"From the representation of one image at different scales, which is technically done by computing a pyramid of downscaled images at different scales, SIFT computes scale-space maxima of the Laplacian representation, which is a specific image energy-based representation, of the image, using so-called differences of Gaussians.",
   "algo1Ap3":"Once this DoG are found, images are searched for local extrema over scale and space. For eg, one pixel in an image is compared with its 8 neighbours as well as 9 pixels in next scale and 9 pixels in previous scales. If it is a local extrema, it is a potential keypoint. It basically means that keypoint is best represented in that scale.",
   "nameIMG1": "step4Algo1img1.jpg",
   "nameIMG2":"step4Algo1img2.jpg",
   	"algo1BTitle":"2- Keypoint Localization",
   	"algo1Bp":"Once potential keypoints locations are found, they have to be refined to get more accurate results. They used Taylor series expansion of scale space to get more accurate location of extrema, and if the intensity at this extrema is less than a threshold value (0.03 as per the paper), it is rejected. DoG has higher response for edges, so edges also need to be removed. For this, a concept similar to Harris corner detector is used. So it eliminates any low-contrast keypoints and edge keypoints and what remains is strong interest points.",
	"algo1CTitle":"3- Orientation Assignment",
    "algo1Cp":"Now an orientation is assigned to each keypoint to achieve invariance to image rotation. A neighbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering 360 degrees is created. (It is weighted by gradient magnitude and gaussian-weighted circular window with σ equal to 1.5 times the scale of keypoint. The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. It contribute to stability of matching.",
	"algo1DTitle":"4- Keypoint Descriptor",
   	"algo1Dp":"Now an orientation is assigned to each keypoint to achieve invariance to image rotation. A neighbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering 360 degrees is created. (It is weighted by gradient magnitude and gaussian-weighted circular window with σ equal to 1.5 times the scale of keypoint. The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. It contribute to stability of matching.",
   "linkAlgo1":"http://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html" ,
 	
   "algo2":"AKAZE",
   "algo2ATitle":"1- Computation of the Nonlinear Scale Space",
   "algo2Ap1":" Similar approach as done in SIFT, discretizing the scale space in logarithmic steps arranged in a series of O octaves and S sub-levels. Note that we always work with the original image resolution, without performing any downsampling at each new octave as done in SIFT. The set of octaves and sub-levels are identified by a discrete octave index o and a sub-level one s. Given an input image, we firstly convolve the image with a Gaussian kernel to reduce noise and possible image artefacts. From that base image we compute the image gradient histogram and obtain a contrast parameter. ",
   "algo2Ap2":"fig :  Comparison between the Gaussian and nonlinear diffusion scale space for several evolution times ti. First Row: Gaussian scale space (linear diffusion). The scale space is formed by convolving the original image with a Gaussian kernel of increasing standard deviation. Second Row: Nonlinear diffusion scale space with conductivity function g3. ",
   "algo2Ap3":"Then, given the contrast parameter and the set of evolution times ti, it is straightforward to build the nonlinear scale space.As it can be observed, Gaussian blurring smoothes for equal all the structures in the image, whereas in the nonlinear scale space strong image edges remain unaffected",

   "nameIMG1Algo2": "step4Algo2img1.png",

   "algo2BTitle":"2- Feature Detection",
   "algo2Bp1":"For detecting points of interest, we compute the response of scale-normalized determinant of the Hessian at multiple scale levels. Given the set of filtered images from the nonlinear scale space, we analyze the detector response at different scale levels. We search for maxima in scale and spatial location. The search for extrema is performed in all the filtered images. For speeding-up the search for extrema, we firstly check the responses over a window of size 3×3 pixels, in order to discard quickly non-maxima responses. Finally, the position of the keypoint is estimated with sub-pixel accuracy. The set of first and second order derivatives are approximated by means of 3 × 3 Scharr filters of different derivative step sizes. Second order derivatives are approximated by using consecutive Scharr filters in the desired coordinates of the derivatives",
	"algo2Bp2":"These filters approximate rotation invariance significantly better than other popular filters such as Sobel filters or standard central differences differentiation. Notice here that although we need to compute multiscale derivatives for every pixel, we save computational efforts in the description step, since we re-use the same set of derivatives that are computed in the detection step.",

	"algo2CTitle":"3-Feature Description",
	"algo2Csub1":"3.1- Finding the Dominant Orientation",
    "algo2Cp1":"For obtaining rotation invariant descriptors, it is necessary to estimate the dominant orientation in a local neighbourhood centered at the keypoint location. Similar to SURF, we find the dominant orientation in a circular area with a sampling step of size. For each of the samples in the circular area, first order derivatives are weighted with a Gaussian centered at the interest point. Then, the derivative responses are represented as points in vector space and the dominant orientation is found by summing the responses within a sliding circle segment covering an angle of π/3. From the longest vector the dominant orientation is obtained.",
    "algo2Csub2":"3.2- Building the Descriptor",
    "algo2Cp2":"We use the M-SURF descriptor adapted to our nonlinear scale space framework. For a detected feature at a certain scale, first order derivatives are computed over a rectangular grid. This grid is divided into 4×4 subregions. The derivative responses in each subregion are weighted with a Gaussian centered on the subregion center and summed into a descriptor vector. Then, each subregion vector is weighted using a Gaussian defined over a mask of 4×4 and centered on the interest keypoint. When considering the dominant orientation of the keypoint, each of the samples in the rectangular grid is rotated according to the dominant orientation. In addition, the derivatives are also computed according to the dominant orientation. Finally, the descriptor vector of length 64 is normalized into a unit vector to achieve invariance to contrast.",
    "linkAlgo2":"https://www.doc.ic.ac.uk/~ajd/Publications/alcantarilla_etal_eccv2012.pdf",

    "otherTitle":"Other algorithms",
    "otherFeature1":"Feature Detector",
    "oFA1": "FAST – FastFeatureDetector",
    "oFA1": "STAR – StarFeatureDetector",
    "oFA1": "SIFT",
    "oFA2": "SURF ",
    "oFA2": "ORB",
    "oFA4": "BRISK",
    "oFA5": "MSER",
    "oFA6": "GFTT – GoodFeaturesToTrackDetector",
    "oFA7": "HARRIS – GoodFeaturesToTrackDetector with Harris detector enabled",
    "oFA8": "Dense – DenseFeatureDetector",
    "oFA9": "SimpleBlob – SimpleBlobDetector",
    "otherFeature2":"Feature Detector",
    "oFB1": "SIFT",
    "oFB2": "SURF",
    "oFB3": "BRIEF – BriefDescriptorExtractor",
    "oFB4": "BRISK ",
    "oFB5": "ORB",
    "oFB6": "FREA",
    "oFB7": "MOPS - Multiscale Oriented PatcheS descriptor",


   "" : ""
}